{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_tensor_type(torch.BFloat16Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama Architecture Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4096\n",
    "ffnn_dim = 14336\n",
    "n_encoders = 32\n",
    "n_heads = 32\n",
    "n_kv_heads = 8\n",
    "vocab_size = 128256\n",
    "norm_eps = 1e-05\n",
    "rope_theta = 500000.0\n",
    "max_batch_size = 4\n",
    "max_seq_length = 128\n",
    "n_kv_heads_rep = n_heads // n_kv_heads\n",
    "head_dim = d_model // n_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMS Norm and Rotary Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4096])\n",
      "------------------------------\n",
      "torch.Size([2, 8, 32, 128])\n",
      "torch.Size([2, 8, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, norm_eps):\n",
    "        super().__init__()\n",
    "        self.norm_esp = norm_eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim = True) + self.norm_esp)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self._norm(x.float()).type_as(x)\n",
    "        return out * self.weight\n",
    "\n",
    "norm = RMSNorm(d_model, norm_eps)\n",
    "print(norm(torch.randn(2, 8, d_model)).shape)\n",
    "\n",
    "\n",
    "def percompute_freqs_cis(d_model, end, theta = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, d_model, 2)[: (d_model // 2)].float() / d_model))\n",
    "    t = torch.arange(end, device=freqs.device, dtype = torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis, x):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "ip1 = torch.randn(2, 8, n_heads, head_dim)\n",
    "ip2 = torch.randn(2, 8, n_kv_heads, head_dim)\n",
    "\n",
    "freqs_cis1 = percompute_freqs_cis(head_dim, max_seq_length*2, rope_theta)\n",
    "freqs_cis1 = freqs_cis1[0:0+8]\n",
    "out1, out2 = apply_rotary_emb(ip1, ip2, freqs_cis1)\n",
    "print(\"-\"*30)\n",
    "print(out1.shape)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SwiGLU Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 4096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, d_model, ffnn_dim):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, ffnn_dim, bias=False)\n",
    "        self.w3 = nn.Linear(d_model, ffnn_dim, bias=False)\n",
    "        self.w2 = nn.Linear(ffnn_dim, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "        return x\n",
    "    \n",
    "feedfor =FFNN(d_model, ffnn_dim)\n",
    "feedfor(torch.randn(2, 8, d_model)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.einsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
